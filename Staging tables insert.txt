from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, lit
from functools import reduce
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def combine_staging_using_final_table_schema(
    final_table: str,
    date_col: str = "event_time",
    start_year: int = 2005,
    end_year: int = 2024,
):
    spark = SparkSession.builder \
        .appName("Combine Staging to Final Table") \
        .enableHiveSupport() \
        .getOrCreate()

    # Step 1: Read final table schema
    try:
        final_df = spark.sql(f"SELECT * FROM {final_table} WHERE 1=0")
        final_columns = final_df.columns
        logger.info(f"Final table columns: {final_columns}")
    except Exception as e:
        logger.error(f"Error reading final table schema: {e}")
        return

    dfs = []

    # Step 2: Load each staging table with schema alignment
    for year in range(start_year, end_year + 1):
        for month in range(1, 13):
            table_name = f"staging_{year}_{month:02d}"

            # Try to read the staging table
            try:
                df = spark.sql(f"SELECT * FROM {table_name}")
                df_cols = df.columns

                # Add missing columns (if any)
                for col_name in final_columns:
                    if col_name not in df_cols:
                        df = df.withColumn(col_name, lit(None))
                
                # Ensure the schema matches the final table
                df = df.select(final_columns)
                dfs.append(df)

                logger.info(f"Loaded table {table_name} successfully.")
            except Exception as e:
                logger.warning(f"Skipping table {table_name} (Error: {e})")

    if not dfs:
        logger.error("No tables loaded. Exiting.")
        return

    # Step 3: Combine, deduplicate, and add partition
    combined_df = reduce(lambda a, b: a.unionByName(b), dfs)
    combined_df = combined_df.withColumn("date_partition", to_date(col(date_col)))
    deduped_df = combined_df.dropDuplicates()

    # Step 4: Write to final table (overwrite partitioned)
    try:
        deduped_df.createOrReplaceTempView("staging_all")
        spark.sql(f"""
            INSERT OVERWRITE TABLE {final_table}
            PARTITION (date_partition)
            SELECT * FROM staging_all
        """)
        logger.info("Successfully inserted data into the final Hive table.")
    except Exception as e:
        logger.error(f"Error writing to final table: {e}")

