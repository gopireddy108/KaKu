#!/bin/bash

# Configuration
SPARK_HOME=/path/to/spark
PYSPARK_SCRIPT=your_script.py
CLUSTER_CONFIG="--master yarn --deploy-mode cluster"
LOG_DIR=./spark_logs
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# Create log directory if it doesn't exist
mkdir -p $LOG_DIR

# Submit the PySpark job and capture the output
submit_output=$($SPARK_HOME/bin/spark-submit $CLUSTER_CONFIG $PYSPARK_SCRIPT 2>&1)

# Extract application ID
app_id=$(echo "$submit_output" | grep -oP 'application_\d+_\d+')

if [ -z "$app_id" ]; then
    echo "Failed to submit application or retrieve application ID"
    echo "$submit_output"
    exit 1
fi

echo "Submitted application ID: $app_id"

# Function to check application status
get_application_status() {
    yarn application -status $app_id 2>/dev/null | grep "State" | awk '{print $3}'
}

# Wait for application to finish
echo "Waiting for application to finish..."
while true; do
    status=$(get_application_status)
    
    if [[ "$status" == "FINISHED" || "$status" == "FAILED" || "$status" == "KILLED" ]]; then
        echo "Application $app_id has $status"
        break
    fi
    
    sleep 5
done

# Retrieve only the user logs (stdout)
echo "Retrieving logs for $app_id..."
yarn logs -applicationId $app_id > $LOG_DIR/${app_id}_full.log 2>/dev/null

# Extract only the stdout content (user logs)
awk '/^LogType:stdout/,/^LogType:stderr/' $LOG_DIR/${app_id}_full.log | grep -v '^LogType:' > $LOG_DIR/${app_id}_stdout.log

# Clean up full log if you only want stdout
rm $LOG_DIR/${app_id}_full.log

echo "User logs saved to $LOG_DIR/${app_id}_stdout.log"
